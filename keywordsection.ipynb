{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd087b2800b0c1b1ef7516301f78dddd1d50cc2355215b22447e1facc53db262490",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "87b2800b0c1b1ef7516301f78dddd1d50cc2355215b22447e1facc53db262490"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CIK: 1750\n\nLINE LIST LENGTH: 67352\n0\n0\nWORD: COVID-19\nbefore:  \n \n\n\nmatch:  ['During the COVID-19 pandemic, Abbott has taken aggressive steps to limit exposure and enhance the safety of facilities for its employees, including implementing mandatory temperature screening and social distancing,\\n'] \n\n\nafter:  providing and requiring the use of personal protective equipment, and at most U.S. facilities, onsite COVID-19 testing.\n\n----------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cb126e5e4c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CURRENTLY THIS IS A PROOF OF CONCEPT\n",
    "\n",
    "'''\n",
    "TO-DO LIST:\n",
    "1. read the document into a list with the lines being seperated\n",
    "2. find the positions of every line using indexes\n",
    "3. search each index in the list for the keywords\n",
    "4. when the keyword is found, take note of the index it was found in\n",
    "5. compile a bunch of keyword indexes into a list and compare their locations\n",
    "6. apply a filter based on how they are sorted\n",
    "    6a. ex: supply chain first, then covid 19. if they are 2 or however many paragrpahs apart, etc.\n",
    "\n",
    "\n",
    "\n",
    "CURRENTLY THE INDEX IS BROKEN, IT SHOWS THE COLUMN THE WORD IS ON.\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "documents = []\n",
    "document_name = []\n",
    "wordlist = ['COVID-19', 'pandemic', 'Coronavirus', 'supply chain']\n",
    "paragraph_list = []\n",
    "\n",
    "for root, dirs, files, in os.walk('2020/'):\n",
    "    for file in files:\n",
    "        if file.endswith('.html'):\n",
    "            documents.append(os.path.join(root, file))\n",
    "            document_name.append(file)\n",
    "\n",
    "\n",
    "for i in range(0, 39):\n",
    "\n",
    "    current_cik = open('ANNCRSPcik210411.txt', 'r').readlines()[i]\n",
    "    print(\"CIK:\", current_cik)\n",
    "\n",
    "    path = str(documents[i])\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        line_list = f.readlines()\n",
    "        full_text = f.read()\n",
    "        print(\"LINE LIST LENGTH:\", len(line_list))\n",
    "        document_soup = BeautifulSoup(full_text, 'lxml')\n",
    "\n",
    "        page = document_soup.findAll('div')\n",
    "\n",
    "        for x in page:\n",
    "            paragraphs = document_soup.findAll('p')\n",
    "            for y in range(len(paragraphs)):\n",
    "                paragraph_list.append(paragraphs[y].get_text())\n",
    "        print(len(page))\n",
    "        print(len(paragraph_list))\n",
    "\n",
    "        for x in range(len(wordlist)):\n",
    "            print('WORD:', str(wordlist[x]))\n",
    "            for y in range(len(line_list)):\n",
    "                if wordlist[x] in line_list[y]:\n",
    "                    soup = BeautifulSoup(line_list[y], 'lxml')\n",
    "                    before_soup = BeautifulSoup(line_list[y-1], 'lxml')\n",
    "                    after_soup = BeautifulSoup(line_list[y+1], 'lxml')\n",
    "\n",
    "                    try:\n",
    "                        before_found = before_soup.find(\"p\").getText()\n",
    "                        print(\"before: \",before_found, '\\n\\n')\n",
    "                    except:\n",
    "                        print(\"before: NONE\")\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        found = soup.findAll(text = re.compile(wordlist[x]))\n",
    "                        print(\"match: \",found, '\\n\\n')  \n",
    "                    except:\n",
    "                        print(\"match: NONE\")\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        after_found = after_soup.find(\"p\").getText()\n",
    "                        print(\"after: \",after_found)\n",
    "                    except:\n",
    "                        print(\"after: NONE\")\n",
    "                        pass\n",
    "\n",
    "                    print(\"----------------------\")\n",
    "                    time.sleep(20)\n",
    "\n",
    "\n",
    "        '''\n",
    "        for x in range(len(line_list)):\n",
    "            for y in range(len(wordlist)):\n",
    "                if wordlist[y] in line_list[x]:\n",
    "                    soup = BeautifulSoup(line_list[x], 'lxml')\n",
    "                    found = soup.findAll(text = re.compile(wordlist[y]))\n",
    "                    print(\"CIK:\", current_cik)\n",
    "                    print('Word:', str(wordlist[y]))\n",
    "                    print(found, line_list[x].index(wordlist[y]))\n",
    "                    print(\"----------------------\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        line_list = f.readlines()\n",
    "\n",
    "        for x in range(len(line_list)):\n",
    "            for y in range(len(wordlist)):\n",
    "                if wordlist[y] in line_list[x]:\n",
    "                    print(\"CIK:\", current_cik, line_list[x], line_list[x].index(wordlist[y]))\n",
    "                    print(\"--------------\")\n",
    "\n",
    "'''"
   ]
  }
 ]
}