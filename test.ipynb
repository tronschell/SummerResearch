{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "from company_name import *\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"try:\n",
    "    cluster = MongoClient(\"mongodb+srv://tron-schell:Aurf7046@covidresearch0.d3ctd.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\", serverSelectionTimeoutMS=5000)\n",
    "    database = cluster['CovidResearch']\n",
    "    collection = database['entire_doc_employee']\n",
    "    print(\"Connected successfully!!!\")\n",
    "except:\n",
    "    print(\"Could not connect to MongoDB\")\"\"\"\n",
    "\n",
    "\n",
    "#db.entire_doc_employee('CIK', {$or: [{'Secondary Word':\"laid off\"}, {'Secondary Word':\"layoffs\"}, {'Secondary Word':\"layoff\", {'Secondary Word':\"furlough\"}, {'Secondary Word':\"furloughs\"}})\n",
    "\n",
    "\n",
    "documents = []\n",
    "#count_not_found = 0\n",
    "primary_wordlist = ['COVID-19']\n",
    "secondary_wordlist = ['salary reduction', 'reducing', 'benefits']\n",
    "\n",
    "not_found = []\n",
    "foundcount = 1\n",
    "\n",
    "found_lst = []\n",
    "\n",
    "# This finds all of the files with the .txt extension and adds the name of the file to the list called \"documents\"\n",
    "for root, dirs, files, in os.walk('sec-edgar-filings'):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            documents.append(os.path.join(root, file))\n",
    "\n",
    "documents.sort()\n",
    "\n",
    "for doc in range(int(len(documents))):\n",
    "    path = str(documents[doc])\n",
    "\n",
    "    current_cik = os.listdir('sec-edgar-filings')\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        raw_10k = f.read()\n",
    "\n",
    "        #Try to run this, if something happens, go to the next document but also increment the count for the amount of things not found\n",
    "\n",
    "        print('Checking document', documents[doc])\n",
    "        print('Checking number', doc, 'out of', len(documents))\n",
    "        #Add all of the paragraphs to the item_1a_paragrpahs list\n",
    "        doc_content = BeautifulSoup(raw_10k, 'lxml')\n",
    "\n",
    "        #This block of code finds the tag with the most amount of information inside of it and makes it the primary tag for finding things\n",
    "        #This was made instead of having different tables/collections for different tags\n",
    "        p_doc_paragraphs = doc_content.findAll('p')\n",
    "        span_doc_paragraphs = doc_content.findAll('span')\n",
    "        font_doc_paragraphs = doc_content.findAll('font')\n",
    "\n",
    "        doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "        doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "\n",
    "        type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "\n",
    "        doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10k)]\n",
    "        doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10k)]\n",
    "\n",
    "        doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10k)]\n",
    "\n",
    "        document = {}\n",
    "\n",
    "        # Create a loop to go through each section type and save only the 10-K section in the dictionary\n",
    "        for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "            if doc_type == '10-K':\n",
    "                document[doc_type] = raw_10k[doc_start:doc_end]\n",
    "\n",
    "        regex = re.compile(r'((ITEM)|(Item)|(item))\\s?(\\&nbsp;)?(\\d\\w?)[.:-]\\s?(\\&nbsp;)?(\\&nbsp;)?(\\&nbsp;)?(\\&nbsp;)?\\s?(\\w){0,4}', flags =re.S|re.M|re.I)\n",
    "\n",
    "        matches = regex.finditer(document['10-K'])\n",
    "        \n",
    "        \n",
    "        # Create the dataframe\n",
    "        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "        try:\n",
    "            test_df.columns = ['item', 'start', 'end']\n",
    "        \n",
    "            test_df['item'] = test_df.item.str.lower()\n",
    "\n",
    "            test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "            test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "            test_df.replace(' ','',regex=True,inplace=True)\n",
    "            test_df.replace('\\.','',regex=True,inplace=True)\n",
    "            test_df.replace('\\n','',regex=True,inplace=True)\n",
    "            test_df.replace('>','',regex=True,inplace=True)\n",
    "\n",
    "            pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n",
    "\n",
    "            pos_dat.set_index('item', inplace=True)\n",
    "            #print(test_df)\n",
    "            print(pos_dat.head())\n",
    "        except:\n",
    "            print('An error has occured, there is a possibility of an empty dataframe.')\n",
    "            continue\n",
    "        #print('item 1a: ', test_df[test_df['item'].str.contains(r'item1a')])\n",
    "\n",
    "        if (len(p_doc_paragraphs) > len(span_doc_paragraphs)) and (len(p_doc_paragraphs) > len(font_doc_paragraphs)):\n",
    "            print('using p doc paragraphs')\n",
    "            doc_paragraphs = p_doc_paragraphs\n",
    "            tags = 'p'\n",
    "        elif (len(span_doc_paragraphs) > len(p_doc_paragraphs)) and (len(span_doc_paragraphs) > len(font_doc_paragraphs)):\n",
    "            print('using span paragraphs')\n",
    "            doc_paragraphs = span_doc_paragraphs\n",
    "            tags = 'span'\n",
    "        else:\n",
    "            doc_paragraphs = font_doc_paragraphs\n",
    "            tags = 'font'\n",
    "            \"\"\"\n",
    "        \"\"\"\n",
    "        #For every word in the range of the length of the list of \"primary_word list\" run the code underneath\n",
    "        for p_word in range(len(primary_wordlist)):\n",
    "\n",
    "            # For every line in the range of the length of the \"doc_paragraphs\" run the code underneath\n",
    "            for j in range(len(doc_paragraphs)):\n",
    "\n",
    "                # If an instance of a primary word is somewhere in the \"item_1a_paragrpahs list\", then run the code underneath\n",
    "                if primary_wordlist[p_word] in doc_paragraphs[j].get_text():\n",
    "\n",
    "                    found = str(doc_paragraphs[j].get_text())\n",
    "\n",
    "                    try:\n",
    "                        paragraph_position = raw_10k.index(str(found[0:40]))\n",
    "                        print(paragraph_position)\n",
    "                        pos_dat['calc'] = paragraph_position - pos_dat['start']\n",
    "                        #print(pos_dat)\n",
    "                        \n",
    "                        index = pos_dat.index\n",
    "                        condition = pos_dat['calc'] > 0\n",
    "                        items = index[condition]\n",
    "                        items_list = items.tolist()\n",
    "                        print(items_list[-1])\n",
    "\n",
    "                    except:\n",
    "                        print('Could not find the string inside the text. Moving on.')\n",
    "                        continue\n",
    "                    # before_found is the instance before the position the primary word was found in, so in that case -1 the index\n",
    "                    try:\n",
    "                        before_found = str(doc_paragraphs[j-1].get_text())\n",
    "                        while len(before_found) <= 2:\n",
    "                            foundcount +=1\n",
    "                            before_found = str(doc_paragraphs[j-foundcount].get_text())\n",
    "                    except:\n",
    "                        before_found = 'None'\n",
    "\n",
    "                    # after_found is the instance after the position the primary word was found in, so in that case  +1 the index\n",
    "                    try:\n",
    "                        after_found = str(doc_paragraphs[j+1].get_text())\n",
    "                    # If the length of the after found paragrpah is less than or equal to 2 characters it is most likely a space, number, or bullet point. In that case, skip it by incrementint\n",
    "                        while len(after_found) <= 2:\n",
    "                            foundcount +=1\n",
    "                            after_found = str(doc_paragraphs[j+foundcount].get_text())\n",
    "                    except:\n",
    "                        after_found = 'None'\n",
    "\n",
    "                    #For every secondary word in the secondary word list, run the code below\n",
    "                    for s_word in range(len(secondary_wordlist)):  \n",
    "\n",
    "                        if secondary_wordlist[s_word] in found:\n",
    "                            '''print(\"CIK: \", current_cik[doc])\n",
    "                            print('PRIMARY WORD: ', primary_wordlist[p_word])\n",
    "                            print('SECONDARY WORD:', secondary_wordlist[s_word])\n",
    "                            print('ENTIRE DOCUMENT', '\\n\\n')\n",
    "\n",
    "                            print(\"\\tbefore: \",before_found, '\\n\\n')\n",
    "                            print(\"\\tmatch: \",found, '\\n\\n')\n",
    "                            print(\"----------------------\")'''\n",
    "\n",
    "                            found_dict = {\n",
    "                                    \"CIK\" : current_cik[doc],\n",
    "                                    \"Company Name\" : getCompanyName(documents[doc]),\n",
    "                                    \"Item\": 'Entire Document',\n",
    "                                    \"Primary Word\" : primary_wordlist[p_word],\n",
    "                                    \"Secondary Word\" : secondary_wordlist[s_word],\n",
    "                                    \"Tag\": tags,\n",
    "                                    \"match\" : found\n",
    "                                }\n",
    "                            print('added to database')\n",
    "                            '''ids = collection.insert_one(found_dict)\n",
    "                            print('added', ids)'''\n",
    "\n",
    "                        elif secondary_wordlist[s_word] in after_found and secondary_wordlist[s_word] in before_found:\n",
    "\n",
    "                            found_dict = {\n",
    "                                \"CIK\" : current_cik[doc],\n",
    "                                \"Company Name\" : getCompanyName(documents[doc]),\n",
    "                                \"Item\": 'Entire Document',\n",
    "                                \"Primary Word\" : primary_wordlist[p_word],\n",
    "                                \"Secondary Word\" : secondary_wordlist[s_word],\n",
    "                                \"Tag\": tags,\n",
    "                                \"before\": before_found,\n",
    "                                \"match\" : found,\n",
    "                                \"after\" : after_found\n",
    "                            } \n",
    "                            print('added to database')\n",
    "                            '''ids = collection.insert_one(found_dict)\n",
    "                            print('added', ids)'''\n",
    "\n",
    "                        # If there is a secondary word in the before paragraph, then run the code underneath\n",
    "                        elif secondary_wordlist[s_word] in before_found:\n",
    "\n",
    "                            found_dict = {\n",
    "                                    \"CIK\" : current_cik[doc],\n",
    "                                    \"Company Name\" : getCompanyName(documents[doc]),\n",
    "                                    \"Item\": 'Entire Document',\n",
    "                                    \"Primary Word\" : primary_wordlist[p_word],\n",
    "                                    \"Secondary Word\" : secondary_wordlist[s_word],\n",
    "                                    \"Tag\": tags,\n",
    "                                    \"before\": before_found,\n",
    "                                    \"match\" : found\n",
    "                                }\n",
    "                            print('added to database')\n",
    "                            '''ids = collection.insert_one(found_dict)\n",
    "                            print('added', ids)'''\n",
    "\n",
    "                        # Else if there is a secondary word in the after found paragraph, then run the code underneath\n",
    "                        elif secondary_wordlist[s_word] in after_found:\n",
    "\n",
    "                            found_dict = {\"CIK\" : current_cik[doc],\n",
    "                                    \"Company Name\" : getCompanyName(documents[doc]),\n",
    "                                    \"Item\": 'Entire Document',\n",
    "                                    \"Primary Word\" : primary_wordlist[p_word],\n",
    "                                    \"Secondary Word\" : secondary_wordlist[s_word],\n",
    "                                    \"Tag\": tags,\n",
    "                                    \"match\" : found,\n",
    "                                    \"after\" : after_found}\n",
    "                            print('added to database')\n",
    "                            '''ids = collection.insert_one(found_dict)\n",
    "                            print('added', ids)'''\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "                else:\n",
    "                    continue\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking document sec-edgar-filings/0000001750/10-K/0001104659-20-085310/full-submission.txt\n",
      "Checking number 0 out of 622\n",
      "         start     end\n",
      "item                  \n",
      "item6   327024  327031\n",
      "item1   356046  356053\n",
      "item1a  389205  389213\n",
      "item1b  452450  452458\n",
      "item2   454047  454054\n",
      "using p doc paragraphs\n",
      "360485\n",
      "item1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6q/112b_g4d25x30bfwwj9_ngrr0000gn/T/ipykernel_76302/2720567931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    186\u001b[0m                             found_dict = {\n\u001b[1;32m    187\u001b[0m                                     \u001b[0;34m\"CIK\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcurrent_cik\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                                     \u001b[0;34m\"Company Name\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mgetCompanyName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                                     \u001b[0;34m\"Item\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Entire Document'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                                     \u001b[0;34m\"Primary Word\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprimary_wordlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Python/SummerResearch/company_name.py\u001b[0m in \u001b[0;36mgetCompanyName\u001b[0;34m(CIK)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCIK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mCName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ix:nonnumeric'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'dei:EntityRegistrantName'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxData\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxData\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing_instruction_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}